services:

  # ---------------------------
  # Mysql (Hive metastore)
  # ---------------------------
  mysql:
    image: mysql:8
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: metastore
      MYSQL_USER: hive
      MYSQL_PASSWORD: hive
    ports:
      - "3306:3306"
    volumes:
      - mysql-data:/var/lib/mysql

  hive-metastore:
    image: apache/hive:3.1.3
    volumes:
      - ./docker/hive:/opt/hive/conf
      - ./drivers/mysql-connector-j-8.3.0.jar:/opt/hive/lib/mysql.jar
      - ./warehouse:/warehouse
    container_name: hive-metastore
    entrypoint: /bin/bash
    command:
      - -c
      - |
        export HIVE_CONF_DIR=/opt/hive/conf
        /opt/hive/bin/schematool -dbType mysql -validate || /opt/hive/bin/schematool -dbType mysql -initSchema
        /opt/hive/bin/hive --service metastore
    environment:
      SERVICE_NAME: metastore
    depends_on:
      mysql:
        condition: service_started
    ports:
      - "9083:9083"
    healthcheck:
      test: ["CMD", "sh", "-c", "pgrep -f HiveMetaStore"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  # ---------------------------
  # Spark cluster
  # ---------------------------
  spark-master:
    user: "0:0"
    build:
      context: .
      dockerfile: docker/spark/Dockerfile
    command: bash -c "/opt/spark/sbin/start-master.sh && tail -f /dev/null"
    volumes:
      - ./data:/data
      - ./src:/app/src
      - ./warehouse:/warehouse
    ports:
      - "7077:7077"
      - "8080:8080"

  spark-worker:
    user: "0:0"
    build:
      context: .
      dockerfile: docker/spark/Dockerfile
    command: bash -c "/opt/spark/sbin/start-worker.sh spark://spark-master:7077 && tail -f /dev/null"
    volumes:
      - ./data:/data
      - ./warehouse:/warehouse
    depends_on:
      - spark-master
    environment:
      SPARK_WORKER_CORES: "8"
      SPARK_WORKER_MEMORY: "6g"

  spark-thrift:
    user: "0:0"
    build:
      context: .
      dockerfile: docker/spark/Dockerfile
    command: >
      bash -c "
      echo 'Waiting for Hive Metastore to be ready...';
      for i in {1..30}; do
        if timeout 2 bash -c 'cat < /dev/null > /dev/tcp/hive-metastore/9083' 2>/dev/null; then
          echo 'Hive Metastore is ready!';
          break;
        fi;
        echo 'Waiting for Hive Metastore... attempt' \$i;
        sleep 2;
      done;
      /opt/spark/sbin/start-thriftserver.sh --master spark://spark-master:7077 --conf spark.cores.max=1 --conf spark.executor.cores=1 --conf spark.executor.memory=1g --conf spark.dynamicAllocation.enabled=false && tail -f /dev/null
      "
    ports:
      - "10000:10000"
    volumes:
      - ./warehouse:/warehouse
    depends_on:
      spark-master:
        condition: service_started
      hive-metastore:
        condition: service_healthy

  # ---------------------------
  # MinIO (S3 local)
  # ---------------------------
  minio:
    image: minio/minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio123
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ./minio:/data

  # ---------------------------
  # dbt
  # ---------------------------
  dbt:
    build:
      context: .
      dockerfile: docker/dbt/Dockerfile
    volumes:
      - ./dbt:/dbt
      - ./dbt/profiles:/root/.dbt
    depends_on:
      - spark-thrift

  # ---------------------------
  # Airflow metadata DB
  # ---------------------------
  airflow-postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow

  # ---------------------------
  # Airflow webserver
  # ---------------------------
  airflow-init:
    build:
      context: .
      dockerfile: docker/airflow/Dockerfile
    depends_on:
      - airflow-postgres
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./data:/data
    command: >
      bash -c "airflow db migrate && airflow users create \
      --username admin \
      --password admin \
      --firstname admin \
      --lastname admin \
      --role Admin \
      --email admin@example.com"
  airflow:
    build:
      context: .
      dockerfile: docker/airflow/Dockerfile
    command: webserver
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./data:/data
    ports:
      - "8081:8080"
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      airflow-postgres:
        condition: service_started
      spark-master:
        condition: service_started
      spark-thrift:
        condition: service_started

  # ---------------------------
  # Airflow scheduler
  # ---------------------------
  airflow-scheduler:
    build:
      context: .
      dockerfile: docker/airflow/Dockerfile
    command: scheduler
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./data:/data
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      airflow:
        condition: service_started

  # ---------------------------
  # Airflow triggerer (modern)
  # ---------------------------
  airflow-triggerer:
    build:
      context: .
      dockerfile: docker/airflow/Dockerfile
    command: triggerer
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
    volumes:
      - ./airflow/dags:/opt/airflow/dags
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      airflow:
        condition: service_started

volumes:
  mysql-data:
