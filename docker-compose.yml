services:

  # ---------------------------
  # Mysql (Hive metastore)
  # ---------------------------
  mysql:
    image: mysql:8
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: metastore
      MYSQL_USER: hive
      MYSQL_PASSWORD: hive
    ports:
      - "3306:3306"

  hive-metastore:
    image: apache/hive:3.1.3
    volumes:
      - ./warehouse:/opt/spark/warehouse
    container_name: hive-metastore
    command: >
      bash -c "
      schematool -dbType mysql -initSchema &&
      hive --service metastore
      "
    environment:
      SERVICE_NAME: metastore
      HIVE_METASTORE_DB_TYPE: mysql
      HIVE_JDBC_URL: jdbc:mysql://mysql:3306/metastore
      HIVE_JDBC_USERNAME: hive
      HIVE_JDBC_PASSWORD: hive
    depends_on:
      mysql:
        condition: service_started
    ports:
      - "9083:9083"
    healthcheck:
      test: ["CMD", "sh", "-c", "pgrep -f HiveMetaStore"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  # ---------------------------
  # Spark cluster
  # ---------------------------
  spark-master:
    build:
      context: .
      dockerfile: docker/spark/Dockerfile
    command: bash -c "/opt/spark/sbin/start-master.sh && tail -f /dev/null"
    volumes:
      - ./data:/data
      - ./src:/app/src
      - ./warehouse:/opt/spark/warehouse
    ports:
      - "7077:7077"
      - "8080:8080"

  spark-worker:
    build:
      context: .
      dockerfile: docker/spark/Dockerfile
    command: bash -c "/opt/spark/sbin/start-worker.sh spark://spark-master:7077 && tail -f /dev/null"
    volumes:
      - ./data:/data
      - ./warehouse:/opt/spark/warehouse
    depends_on:
      - spark-master
    environment:
      SPARK_WORKER_CORES: "8"
      SPARK_WORKER_MEMORY: "6g"

  spark-thrift:
    build:
      context: .
      dockerfile: docker/spark/Dockerfile
    command: >
      bash -c "
      echo 'Waiting for Hive Metastore to be ready...';
      for i in {1..30}; do
        if timeout 2 bash -c 'cat < /dev/null > /dev/tcp/hive-metastore/9083' 2>/dev/null; then
          echo 'Hive Metastore is ready!';
          break;
        fi;
        echo 'Waiting for Hive Metastore... attempt' \$i;
        sleep 2;
      done;
      /opt/spark/sbin/start-thriftserver.sh --master spark://spark-master:7077 --conf spark.cores.max=1 --conf spark.executor.cores=1 --conf spark.executor.memory=1g --conf spark.dynamicAllocation.enabled=false && tail -f /dev/null
      "
    ports:
      - "10000:10000"
    volumes:
      - ./warehouse:/opt/spark/warehouse
    depends_on:
      spark-master:
        condition: service_started
      hive-metastore:
        condition: service_healthy

  # ---------------------------
  # MinIO (S3 local)
  # ---------------------------
  minio:
    image: minio/minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio123
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ./minio:/data

  # ---------------------------
  # dbt
  # ---------------------------
  dbt:
    build:
      context: .
      dockerfile: docker/dbt/Dockerfile
    volumes:
      - ./dbt:/dbt
      - ./dbt/profiles:/root/.dbt
    depends_on:
      - spark-thrift

  # ---------------------------
  # Airflow metadata DB
  # ---------------------------
  airflow-postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow

  # ---------------------------
  # Airflow webserver
  # ---------------------------
  airflow:
    build:
      context: .
      dockerfile: docker/airflow/Dockerfile
    command: webserver
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./data:/data
    ports:
      - "8081:8080"
    depends_on:
      - airflow-postgres
      - spark-master
      - spark-thrift

  # ---------------------------
  # Airflow scheduler
  # ---------------------------
  airflow-scheduler:
    build:
      context: .
      dockerfile: docker/airflow/Dockerfile
    command: scheduler
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./data:/data
    depends_on:
      - airflow

  # ---------------------------
  # Airflow triggerer (modern)
  # ---------------------------
  airflow-triggerer:
    build:
      context: .
      dockerfile: docker/airflow/Dockerfile
    command: triggerer
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
    volumes:
      - ./airflow/dags:/opt/airflow/dags
    depends_on:
      - airflow
